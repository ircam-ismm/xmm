<!-- HTML header for doxygen 1.8.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>XMM - Probabilistic Models for Motion Recognition and Mapping: Introduction</title>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href='http://fonts.googleapis.com/css?family=Merriweather:400,700,400italic,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Droid+Sans:400,700' rel='stylesheet' type='text/css'>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="jdoxygen.css" rel="stylesheet" type="text/css" />
<link href="jtabs.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <img src="xmm_featured.jpg"/>
  <h1>XMM - Probabilistic Models for Motion Recognition and Mapping</h1>
  <nav>
    <ul>
      <li><a href="index.html">About</a></li>
      <li><a href="GettingStarted.html">Getting Started</a></li>
      <li><a href="Documentation.html">Documentation</a></li>
      <li><a href="Download.html">Download</a></li>
    </ul>
  </nav>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="GettingStarted.html">Getting Started</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#why">Why another HMM Library?</a></li>
<li class="level1"><a href="#fourmodels">Four Models</a></li>
<li class="level1"><a href="#architecture">Architecture</a></li>
<li class="level1"><a href="#relatedpubs">Related Publications</a></li>
</ul>
</div>
<div class="textblock"><p>XMM is a portable, cross-platform C++ library that implements Gaussian Mixture Models and Hidden Markov Models for both recognition and regression. The XMM library was developed with interaction as a central constraint and allows for continuous, real-time use of the proposed methods. </p>
<h1><a class="anchor" id="why"></a>
Why another HMM Library?</h1>
<p>Several general machine learning toolkits have become popular over the years, such as Weka in Java, Sckits-Learn in Python, or more recently MLPack in C++. However, none of the above libraries were adapted for the purpose of this thesis. As a matter of fact, most HMM implementations are oriented towards classification and they often only implement offline inference using the Viterbi algorithm.</p>
<p>In speech processing, the <a href="http://htk.eng.cam.ac.uk/">Hidden Markov Model Toolkit (HTK)</a> has now become a standard in Automatic Speech Recognition, and gave birth to a branch oriented towards synthesis, called <a href="http://hts.sp.nitech.ac.jp/">HTS</a>. Both libraries present many features specific to speech synthesis that do not yet match our use-cases in movement and sound processing, and have a really complex structure that does not facilitate embedding.</p>
<p>Above all, we did not find any library explicitly implementing the Hierarchical HMM, nor the regression methods based on GMMs and HMMs. For these reasons, we decided to start of novel implementation of these methods with the following constraints:</p><ul>
<li><b>Real-Time:</b> Inference must be performed in continuously, meaning that the models must update their internal state and prediction at each new observation to allow continuous recognition and generation.</li>
<li><b>Interactive:</b> The library must be compatible with an interactive learning workflow, that allows users to easily define and edit training sets, train models, and evaluate the results through direct interaction. All models must be able to learn from few examples (possibly a single demonstration).</li>
<li><b>Portable:</b> In order to be integrated within various software, platforms, the library must be portable, cross-platform, and lightweight.</li>
</ul>
<p>We chose C++ that is both efficient and easy to integrate within other software and languages such as Max and Python. We now detail the four models that are implemented to date, the architecture of the library as well as the proposed Max/MuBu implementation with several examples. </p>
<h1><a class="anchor" id="fourmodels"></a>
Four Models</h1>
<p>The implemented models are summarized in Table the following table. Each of the four model addresses a different combination of the multimodal and temporal aspects. We implemented two instantaneous models based on Gaussian Mixture Models and two temporal models with a hierarchical structure, based on an extension of the basic Hidden Markov Model (HMM) formalism. </p><table class="doxtable">
<tr>
<th>\ </th><th>Movement </th><th>Multimodal  </th></tr>
<tr>
<td>Instantaneous </td><td>Gaussian Mixture Model (GMM) </td><td>Gaussian Mixture Regression </td></tr>
</table>
<p>(GMR) Temporal | Hierarchical Hidden Markov Model(HHMM) | Multimodal Hierarchical Hidden Markov Model(MHMM)</p>
<ul>
<li><b>Gaussian Mixture Models (GMMs)</b> are instantaneous movement models. The input data associated to a class defined by the training sets is abstracted by a mixture (i.e. a weighted sum) of Gaussian distributions. This representation allows recognition in the <em>performance</em> phase: for each input frame the model calculates the likelihood of each class (Figure 1 (<b>a</b>)).</li>
<li><b>Gaussian Mixture Regression (GMR)</b> are a straightforward extension of Gaussian Mixture Models used for regression. Trained with multimodal data, GMR allows for predicting the features of one modality (e.g. sound) from the features of another (e.g. movement) through non-linear regression between both feature sets (Figure 1 (<b>b</b>)).</li>
<li><b>Hierarchical HMM (HHMM)</b> integrates a high-level structure that governs the transitions between classical HMM structures representing the temporal evolution of &mdash; low-level &mdash; movement segments. In the <em>performance</em> phase of the system, the hierarchical model estimates the likeliest gesture according to the transitions defined by the user. The system continuously estimates the likelihood for each model, as well as the time progression within the original training phrases (Figure 1 (<b>c</b>)).</li>
<li><b>Multimodal Hierarchical HMM (MHMM)</b> allows for predicting a stream of sound parameters from a stream of movement features. It simultaneously takes into account the temporal evolution of movement and sound as well as their dynamic relationship according to the given example phrases. In this way, it guarantees the temporal consistency of the generated sound, while realizing the trained temporal movement-sound mappings (Figure 1 (<b>d</b>)).</li>
</ul>
<div class="image">
<img src="xmm_models.jpg" alt="xmm_models.jpg"/>
<div class="caption">
Figure 1: Schematic Representation of the 4</div></div>
<p> implemented models"</p>
<h1><a class="anchor" id="architecture"></a>
Architecture</h1>
<p>Our implementation has a particular attention to the interactive training procedure, and to the respect of the real-time constraints of the <em>performance</em> mode. The library is built upon four components representing phrases, training sets, models and model groups, as represented on Figure 2. A phrase is a multimodal data container used to store training examples. A training set is used to aggregate phrases associated with labels. It provides a set of function for interactive recording, editing and annotation of the phrases. Each instance of a model is connected to a training set that provides access to the training phrases. Performance functions are designed for real-time usage, updating the internal state of the model and the results for each new observation of a new movement. The library is portable and cross-platform. It defines a specific format for exchanging trained models, and provides Python bindings for scripting purpose or offline processing. </p><div class="image">
<img src="xmm_architecture.jpg" alt="xmm_architecture.jpg"/>
<div class="caption">
Figure 2: Architecture of the XMM library</div></div>
 <h1><a class="anchor" id="relatedpubs"></a>
Related Publications</h1>
<p>J. Francoise, N. Schnell, R. Borghesi, and F. Bevilacqua, Probabilistic Models for Designing Motion and Sound Relationships. In Proceedings of the 2014 International Conference on New Interfaces for Musical Expression, NIME’14, London, UK, 2014. <a href="http://julesfrancoise.com/blog/wp-content/uploads/2014/06/Fran%C3%A7oise-et-al.-2014-Probabilistic-Models-for-Designing-Motion-and-Sound-Relationships.pdf?1ce945">Download</a></p>
<p>J. Francoise, N. Schnell, and F. Bevilacqua, A Multimodal Probabilistic Model for Gesture-based Control of Sound Synthesis. In Proceedings of the 21st ACM international conference on Multimedia (MM’13), Barcelona, Spain, 2013. <a href="http://architexte.ircam.fr/textes/Francoise13b/index.pdf">Download</a></p>
<center>Prev: <a class="el" href="index.html#mainpage">Home</a> | Next: <a class="el" href="Compilation.html">Compilation and Usage</a>.</center> </div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.9.1-->
</body>
</html>
